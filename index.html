<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>RITUAL</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <img src="static/images/yagna.png" alt="RITUAL" style="width: 10%;"/>
          <h1 class="title is-1 publication-title">RITUAL: Random Image Transformations as a Universal Anti-hallucination Lever in Large Vision Language Models</h1>
          <div class="is-size-5 publication-authors">
            <!-- Paper authors -->
            <span class="author-block">
              <a href="https://sangminwoo.github.io/" target="_blank"><b style="color:#FF6347; font-weight:normal">Sangmin Woo</b></a><sup>*</sup>,
            </span>
            <span class="author-block">
              <b style="color:#FF6347; font-weight:normal">Jaehyuk Jang</b><sup>*</sup>,
            </span>
            <span class="author-block">
              <b style="color:#FF6347; font-weight:normal">Donguk Kim</b><sup>*</sup>,
            </span>
            <span class="author-block">
              <b style="color:#FF6347; font-weight:normal">Yubin Choi</b>,
            </span>
            <span class="author-block">
              <b style="color:#FF6347; font-weight:normal">Changick Kim</b>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block">KAIST</span>
            <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution</small></span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- Arxiv PDF link -->
              <span class="link-block">
                <a href="https://arxiv.org/pdf/2405.17821.pdf" target="_blank" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>

              <!-- Github link -->
              <span class="link-block">
                <a href="https://github.com/sangminwoo/RITUAL" target="_blank" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                </a>
              </span>

              <!-- ArXiv abstract Link -->
              <span class="link-block">
                <a href="https://arxiv.org/abs/2405.17821" target="_blank" class="external-link button is-normal is-rounded is-dark">
                <span class="icon">
                  <i class="ai ai-arxiv"></i>
                </span>
                <span>arXiv</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>



<!-- Teaser video-->
<!-- <section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video poster="" id="tree" autoplay controls muted loop height="100%">
        <source src="static/videos/banner_video.mp4"
        type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        Aliquam vitae elit ullamcorper tellus egestas pellentesque. Ut lacus tellus, maximus vel lectus at, placerat pretium mi. Maecenas dignissim tincidunt vestibulum. Sed consequat hendrerit nisl ut maximus. 
      </h2>
    </div>
  </div>
</section> -->
<!-- End teaser video -->

<!-- Teaser image-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img src="static/images/overview.png" alt="Overview" style="width: 80%; height: auto; display: block; margin: auto;"/>
      <h2 class="subtitle has-text-centered">
        <strong>TL;DR.</strong>
        RITUAL is a simple yet effective anti-hallucination approach for LVLMs. Our RITUAL method leverages basic image transformations (e.g., vertical and horizontal flips) to enhance LVLM accuracy without external models or training. By integrating transformed and original images, RITUAL significantly reduces hallucinations in both discriminative tasks and descriptive tasks. Using both versions together enables the model to refine predictions, reducing errors and boosting correct responses.
      </h2>
    </div>
  </div>
</section>
<!-- End teaser image -->


<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Recent advancements in Large Vision Language Models (LVLMs) have revolutionized how machines understand and generate textual responses based on visual inputs, yet they often produce "hallucinatory" outputs that misinterpret visual information, posing challenges in reliability and trustworthiness. We propose RITUAL, a simple decoding method that reduces hallucinations by leveraging randomly transformed images as complementary inputs during decoding, adjusting the output probability distribution without additional training or external models. Our key insight is that random transformations expose the model to diverse visual perspectives, enabling it to correct misinterpretations that lead to hallucinations. Specifically, when a model hallucinates based on the original image, the transformed images---altered in aspects such as orientation, scale, or color---provide alternative viewpoints that help recalibrate the model's predictions. By integrating the probability distributions from both the original and transformed images, RITUAL effectively reduces hallucinations. To further improve reliability and address potential instability from arbitrary transformations, we introduce RITUAL+, an extension that selects image transformations based on self-feedback from the LVLM. Instead of applying transformations randomly, RITUAL+ uses the LVLM to evaluate and choose transformations that are most beneficial for reducing hallucinations in a given context. This self-adaptive approach mitigates the potential negative impact of certain transformations on specific tasks, ensuring more consistent performance across different scenarios. Experiments demonstrate that RITUAL and RITUAL+ significantly reduce hallucinations across several object hallucination benchmarks.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->


<!-- Teaser image-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img src="static/images/method.png" alt="RITUAL"/>
      <h2 class="subtitle has-text-centered">
        <strong>RITUAL.</strong>
        At each timestep t, LVLM auto-regressively samples a response Î·t given a visual input, a textual query, and previously generated tokens. When conditioned on the original image V, the probabilities for Blue (correct) and Red (hallucinated) responses are similar, which can lead to the hallucinated response being easily sampled. RITUAL leverages an additional probability distribution conditioned on the transformed image V^(T), where the likelihood of hallucination is significantly reduced. Consequently, the response is sampled from a linear combination of the two probability distributions, ensuring more accurate and reliable outputs.
      </h2>
    </div>
  </div>
</section>
<!-- End teaser image -->
 

<!-- Teaser image-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img src="static/images/ritual+.png" alt="RITUAL+" style="width: 80%; height: auto; display: block; margin: auto;"/>
      <h2 class="subtitle has-text-centered">
        <strong>RITUAL+.</strong>
        In <strong>RITUAL</strong>, the original image V undergoes random transformations, generating a transformed image. In <strong>RITUAL+</strong>, the model evaluates various potential transformations and selects the most beneficial one to improve answer accuracy within the given context, further refining reliability. These transformed images serve as complementary inputs, enabling the model to incorporate multiple visual perspectives to reduce hallucinations.
      </h2>
    </div>
  </div>
</section>
<!-- End teaser image -->


<!-- <section class="hero is-small">
  <div class="container">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
      <h2 class="title is-3">Intriguing impact of random image transformations on LVLMs</h2>
        <div class="hero-body">
          <img src="static/images/motivation.png" alt="Motivation"/>
          <h2 class="subtitle has-text-centered">
          <strong>(Left)</strong> Using the randomly transformed image (V^(T)) as a visual input to LVLMs results in lower performance compared to using the original image (V).
          <strong>(Right)</strong> However, when these two images are combined, an intriguing phenomenon is observed: cases incorrectly predicted with the original image are now correctly predicted. (i) Although V^(T) alone does not yield a correct answer, it reduces the likelihood of a hallucinated answer and increases the likelihood of a correct answer. (ii) In some cases, V^(T) strongly aligns with the correct answer, leading to accurate answers.
          </h2>
        </div>
      </div>
    </div>
  </div>
</section> -->



<!-- <section class="hero is-small">
  <div class="container">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
      <h2 class="title is-3">Comparison with Contrastive Decoding Methods</h2>
        <div class="hero-body">
          <img src="static/images/teaser.png" alt="Comparison"/>
          <h2 class="subtitle has-text-centered">
          Unlike contrastive decoding methods, which contrast the conditional probability given the original image (V) to that given a diffused (or absent) image (Vâ²), we leverage both the original image (V) and a randomly transformed image (V^(T)) in a complementary manner. While simple, RITUAL achieves state-of-the-art performance on multiple hallucination benchmarks, including the POPE.
          </h2>
        </div>
      </div>
    </div>
  </div>
</section> -->



<section class="hero is-small">
  <div class="container">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
      <h2 class="title is-3">POPE Results</h2>
        <div class="hero-body">
          <img src="static/images/pope.png" alt="POPE" style="width: 90%;"/>
          <h2 class="subtitle has-text-centered">
          RITUAL consistently outperforms the contrastive decoding baselines: VCD and M3ID. Moreover, RITUAL is shown to be compatible with both VCD and M3ID, leading to further performance improvements in most configurations. VCD and M3ID are reproduced within our evaluation setting.
          </h2>
        </div>
      </div>
    </div>
  </div>
</section>



<section class="hero is-small">
  <div class="container">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
      <h2 class="title is-3">MME-Fullset Results</h2>
        <div class="hero-body">
          <img src="static/images/mme-fullset.png" alt="MME-Fullset" style="width: 90%;"/>
          <h2 class="subtitle has-text-centered">
          When equipped with RITUAL, LLaVA-1.5 performs best in 12 out of 14 categories, while InstructBLIP excels in 11 categories. RITUAL not only reduces hallucinations but also enhances the general capabilities of LVLMs.
          </h2>
        </div>
      </div>
    </div>
  </div>
</section>



<section class="hero is-small">
  <div class="container">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
      <h2 class="title is-3">MME-Hallucination Results</h2>
        <div class="hero-body">
          <img src="static/images/mme-hallucination.png" alt="MME-Hallucination" style="width: 90%;"/>
          <h2 class="subtitle has-text-centered">
          RITUAL effectively mitigates hallucinations at both the object and attribute levels, outperforming contrastive decoding methods in Total Score.
          </h2>
        </div>
      </div>
    </div>
  </div>
</section>



<section class="hero is-small">
  <div class="container">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
      <h2 class="title is-3">CHAIR Results</h2>
        <div class="hero-body">
          <img src="static/images/chair.png" alt="CHAIR" style="width: 40%;"/>
          <h2 class="subtitle has-text-centered">
          RITUAL significantly reduces object hallucinations in caption generation compared to VCD and M3ID. It can also boost performance when combined with these baselines. The number of max new tokens is set to 64.
          </h2>
        </div>
      </div>
    </div>
  </div>
</section>




<!-- Image carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Qualitative Results</h2>
          <div id="results-carousel" class="carousel results-carousel">
            <div class="item">
              <img src="static/images/llava_bench_.png" alt="LLaVA Bench Results"/>
              <h2 class="subtitle has-text-centered">
              <strong>LLaVA-Bench.</strong>
              Hallucinations are highlighted in red.
            </div>
            <div class="item">
              <img src="static/images/llava_bench_appendix.png" alt="LLaVA Bench Results"/>
              <h2 class="subtitle has-text-centered">
              <strong>LLaVA-Bench.</strong>
              Hallucinations are highlighted in red.
              </h2>
            </div>
            <div class="item">
              <img src="static/images/Appendix_CHAIR_.png" alt="CHAIR"/>
              <h2 class="subtitle has-text-centered">
              <strong>CHAIR.</strong>
              Hallucinations are highlighted in red.
              </h2>
            </div>
            <div class="item">
              <img src="static/images/Appendix_POPE_.png" alt="POPE"/>
              <h2 class="subtitle has-text-centered">
              <strong>POPE.</strong>
              Hallucinations are highlighted in red.
              </h2>
            </div>
            <div class="item">
              <img src="static/images/Appendix_MME_.png" alt="MME"/>
              <h2 class="subtitle has-text-centered">
              <strong>MME.</strong>
              Hallucinations are highlighted in red.
              </h2>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End image carousel -->



<!-- Youtube video -->
<!-- <section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Video Presentation</h2>
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <div class="publication-video">
            <iframe src="https://www.youtube.com/embed/JkaxUblCGz0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
          </div>
        </div>
      </div>
    </div>
  </div>
</section> -->
<!-- End youtube video -->



<!-- Video carousel -->
<!-- <section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Another Carousel</h2>
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-video1">
          <video poster="" id="video1" autoplay controls muted loop height="100%">
            <source src="static/videos/carousel1.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item item-video2">
          <video poster="" id="video2" autoplay controls muted loop height="100%">
            <source src="static/videos/carousel2.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item item-video3">
          <video poster="" id="video3" autoplay controls muted loop height="100%">\
            <source src="static/videos/carousel3.mp4"
            type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section> -->
<!-- End video carousel -->



<!-- Paper poster -->
<!-- <section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title">Poster</h2>
      <iframe  src="static/pdfs/sample.pdf" width="100%" height="550">
          </iframe>
      </div>
    </div>
  </section> -->
<!--End paper poster -->



<!--BibTex citation -->
<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>
@article{woo2024ritual,
  title={RITUAL: Random Image Transformations as a Universal Anti-hallucination Lever in Large Vision Language Models}, 
  author={Woo, Sangmin and Jang, Jaehyuk and Kim, Donguk and Choi, Yubin and Kim, Changick},
  journal={arXiv preprint arXiv:2405.17821},
  year={2024},
}
    </code></pre>
  </div>
</section>
<!--End BibTex citation -->



<footer class="footer">
<div class="container">
  <div class="columns is-centered">
    <div class="column is-8">
      <div class="content">
        <p>
          This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from theÂ <a href="https://nerfies.github.io" target="_blank">Nerfies</a>Â project page.
          This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative Commons Attribution-ShareAlike 4.0 International License</a>.
        </p>
      </div>
    </div>
  </div>
</div>
</footer>



<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

  <!-- End of Statcounter Code -->



</body>
</html>
