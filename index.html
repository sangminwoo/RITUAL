<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>RITUAL</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <img src="static/images/yagna.png" alt="RITUAL" style="width: 10%;"/>
          <h1 class="title is-1 publication-title">RITUAL: Random Image Transformations as a Universal Anti-hallucination Lever in LVLMs</h1>
          <div class="is-size-5 publication-authors">
            <!-- Paper authors -->
            <span class="author-block">
              <a href="https://sangminwoo.github.io/" target="_blank">Sangmin Woo</a><sup>*</sup>,
            </span>
            <span class="author-block">
              Jaehyuk Jang<sup>*</sup>,
            </span>
            <span class="author-block">
              Donguk Kim<sup>*</sup>,
            </span>
            <span class="author-block">
              Yubin Choi,
            </span>
            <span class="author-block">
              Changick Kim
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block">KAIST</span>
            <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution</small></span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- Arxiv PDF link -->
              <span class="link-block">
                <a href="https://arxiv.org/pdf/2405.17821.pdf" target="_blank" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>

              <!-- Github link -->
              <span class="link-block">
                <a href="https://github.com/RITUAL" target="_blank" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                </a>
              </span>

              <!-- ArXiv abstract Link -->
              <span class="link-block">
                <a href="https://arxiv.org/abs/2405.17821" target="_blank" class="external-link button is-normal is-rounded is-dark">
                <span class="icon">
                  <i class="ai ai-arxiv"></i>
                </span>
                <span>arXiv</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>



<!-- Teaser video-->
<!-- <section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video poster="" id="tree" autoplay controls muted loop height="100%">
        <source src="static/videos/banner_video.mp4"
        type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        Aliquam vitae elit ullamcorper tellus egestas pellentesque. Ut lacus tellus, maximus vel lectus at, placerat pretium mi. Maecenas dignissim tincidunt vestibulum. Sed consequat hendrerit nisl ut maximus. 
      </h2>
    </div>
  </div>
</section> -->
<!-- End teaser video -->

<!-- Teaser image-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <h2 class="title is-3">Overview</h2>
    <div class="hero-body">
      <img src="static/images/overview.png" alt="Overview"/>
      <h2 class="subtitle has-text-centered">
        At each timestep t, LVLM auto-regressively samples a response ηt given a visual input, a textual query, and previously generated tokens. When conditioned on the original image V, the probabilities for Blue (correct) and Red (hallucinated) responses are similar, which can lead to the hallucinated response being easily sampled. RITUAL leverages an additional probability distribution conditioned on the transformed image V^(T), where the likelihood of hallucination is significantly reduced. Consequently, the response is sampled from a linear combination of the two probability distributions, ensuring more accurate and reliable outputs.
      </h2>
    </div>
  </div>
</section>
<!-- End teaser image -->



<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Recent advancements in Large Vision Language Models (LVLMs) have revolutionized how machines understand and generate textual responses based on visual inputs. Despite their impressive capabilities, they often produce "hallucinatory" outputs that do not accurately reflect the visual information, posing challenges in reliability and trustworthiness. Current methods such as contrastive decoding have made strides in addressing these issues by contrasting the original probability distribution of generated tokens with distorted counterparts; yet, generating visually-faithful outputs remains a challenge. In this work, we shift our focus to the opposite: What could serve as a complementary enhancement to the original probability distribution? We propose a simple, training-free method termed <strong>RITUAL</strong> to enhance robustness against hallucinations in LVLMs. Our approach employs random image transformations as complements to the original probability distribution, aiming to mitigate the likelihood of hallucinatory visual explanations by enriching the model’s exposure to varied visual scenarios. Our empirical results show that while the isolated use of transformed images initially degrades performance, strategic implementation of these transformations can indeed serve as effective complements. Notably, our method is compatible with current contrastive decoding methods and does not require external models or costly self-feedback mechanisms, making it a practical addition. In experiments, RITUAL significantly outperforms existing contrastive decoding methods across several object hallucination benchmarks, including POPE, CHAIR, and MME.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->



<section class="hero is-small">
  <div class="container">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
      <h2 class="title is-3">Intriguing impact of random image transformations on LVLMs</h2>
        <div class="hero-body">
          <img src="static/images/motivation.png" alt="Motivation"/>
          <h2 class="subtitle has-text-centered">
          <strong>(Left)</strong> Using the randomly transformed image (V^(T)) as a visual input to LVLMs results in lower performance compared to using the original image (V).
          <strong>(Right)</strong> However, when these two images are combined, an intriguing phenomenon is observed: cases incorrectly predicted with the original image are now correctly predicted. (i) Although V^(T) alone does not yield a correct answer, it reduces the likelihood of a hallucinated answer and increases the likelihood of a correct answer. (ii) In some cases, V^(T) strongly aligns with the correct answer, leading to accurate answers.
          </h2>
        </div>
      </div>
    </div>
  </div>
</section>



<section class="hero is-small is-light">
  <div class="container">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
      <h2 class="title is-3">Comparison with Contrastive Decoding Methods</h2>
        <div class="hero-body">
          <img src="static/images/teaser.png" alt="Comparison"/>
          <h2 class="subtitle has-text-centered">
          Unlike contrastive decoding methods, which contrast the conditional probability given the original image (V) to that given a diffused (or absent) image (V′), we leverage both the original image (V) and a randomly transformed image (V^(T)) in a complementary manner. While simple, RITUAL achieves state-of-the-art performance on multiple hallucination benchmarks, including the POPE.
          </h2>
        </div>
      </div>
    </div>
  </div>
</section>



<section class="hero is-small">
  <div class="container">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
      <h2 class="title is-3">POPE Results</h2>
        <div class="hero-body">
          <img src="static/images/pope.png" alt="POPE" style="width: 90%;"/>
          <h2 class="subtitle has-text-centered">
          RITUAL consistently outperforms the contrastive decoding baselines: VCD and M3ID. Moreover, RITUAL is shown to be compatible with both VCD and M3ID, leading to further performance improvements in most configurations. VCD and M3ID are reproduced within our evaluation setting.
          </h2>
        </div>
      </div>
    </div>
  </div>
</section>



<section class="hero is-small">
  <div class="container">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
      <h2 class="title is-3">MME-Fullset Results</h2>
        <div class="hero-body">
          <img src="static/images/mme-fullset.png" alt="MME-Fullset" style="width: 90%;"/>
          <h2 class="subtitle has-text-centered">
          When equipped with RITUAL, LLaVA-1.5 performs best in 12 out of 14 categories, while InstructBLIP excels in 11 categories. RITUAL not only reduces hallucinations but also enhances the general capabilities of LVLMs.
          </h2>
        </div>
      </div>
    </div>
  </div>
</section>



<section class="hero is-small">
  <div class="container">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
      <h2 class="title is-3">MME-Hallucination Results</h2>
        <div class="hero-body">
          <img src="static/images/mme-hallucination.png" alt="MME-Hallucination" style="width: 90%;"/>
          <h2 class="subtitle has-text-centered">
          RITUAL effectively mitigates hallucinations at both the object and attribute levels, outperforming contrastive decoding methods in Total Score.
          </h2>
        </div>
      </div>
    </div>
  </div>
</section>



<section class="hero is-small">
  <div class="container">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
      <h2 class="title is-3">CHAIR Results</h2>
        <div class="hero-body">
          <img src="static/images/chair.png" alt="CHAIR" style="width: 40%;"/>
          <h2 class="subtitle has-text-centered">
          RITUAL significantly reduces object hallucinations in caption generation compared to VCD and M3ID. It can also boost performance when combined with these baselines. The number of max new tokens is set to 64.
          </h2>
        </div>
      </div>
    </div>
  </div>
</section>




<!-- Image carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Qualitative Results</h2>
          <div id="results-carousel" class="carousel results-carousel">
            <div class="item">
              <img src="static/images/llava_bench.png" alt="LLaVA Bench Results"/>
              <h2 class="subtitle has-text-centered">
              <strong>LLaVA-Bench.</strong>
              Hallucinations are highlighted in red.
            </div>
            <div class="item">
              <img src="static/images/llava_bench_appendix.png" alt="LLaVA Bench Results"/>
              <h2 class="subtitle has-text-centered">
              <strong>LLaVA-Bench.</strong>
              Hallucinations are highlighted in red.
              </h2>
            </div>
            <div class="item">
              <img src="static/images/Appendix_CHAIR.png" alt="CHAIR"/>
              <h2 class="subtitle has-text-centered">
              <strong>CHAIR.</strong>
              Hallucinations are highlighted in red.
              </h2>
            </div>
            <div class="item">
              <img src="static/images/Appendix_POPE.png" alt="POPE"/>
              <h2 class="subtitle has-text-centered">
              <strong>POPE.</strong>
              Hallucinations are highlighted in red.
              </h2>
            </div>
            <div class="item">
              <img src="static/images/Appendix_MME.png" alt="MME"/>
              <h2 class="subtitle has-text-centered">
              <strong>MME.</strong>
              Hallucinations are highlighted in red.
              </h2>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End image carousel -->



<!-- Youtube video -->
<!-- <section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Video Presentation</h2>
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <div class="publication-video">
            <iframe src="https://www.youtube.com/embed/JkaxUblCGz0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
          </div>
        </div>
      </div>
    </div>
  </div>
</section> -->
<!-- End youtube video -->



<!-- Video carousel -->
<!-- <section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Another Carousel</h2>
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-video1">
          <video poster="" id="video1" autoplay controls muted loop height="100%">
            <source src="static/videos/carousel1.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item item-video2">
          <video poster="" id="video2" autoplay controls muted loop height="100%">
            <source src="static/videos/carousel2.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item item-video3">
          <video poster="" id="video3" autoplay controls muted loop height="100%">\
            <source src="static/videos/carousel3.mp4"
            type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section> -->
<!-- End video carousel -->



<!-- Paper poster -->
<!-- <section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title">Poster</h2>
      <iframe  src="static/pdfs/sample.pdf" width="100%" height="550">
          </iframe>
      </div>
    </div>
  </section> -->
<!--End paper poster -->



<!--BibTex citation -->
<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>
@article{{woo2024ritual,
  title={RITUAL: Random Image Transformations as a Universal Anti-hallucination Lever in LVLMs}, 
  author={Woo, Sangmin and Jang, Jaehyuk and Kim, Donguk and Choi, Yubin and Kim, Changick},
  journal={arXiv preprint arXiv:2405.17821},
  year={2024},
}
    </code></pre>
  </div>
</section>
<!--End BibTex citation -->



<footer class="footer">
<div class="container">
  <div class="columns is-centered">
    <div class="column is-8">
      <div class="content">
        <p>
          This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
          This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative Commons Attribution-ShareAlike 4.0 International License</a>.
        </p>
      </div>
    </div>
  </div>
</div>
</footer>



<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

  <!-- End of Statcounter Code -->



</body>
</html>